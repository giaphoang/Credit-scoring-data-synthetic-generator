# Credit Data Generator ğŸ¦ğŸ“Š

> **Synthetic credit datasets at scale, powered by a custom GAN and a cloudâ€‘native bigâ€‘data pipeline.**

![CI](https://github.com/yourâ€‘org/creditâ€‘dataâ€‘generator/actions/workflows/ci.yml/badge.svg)
![License](https://img.shields.io/github/license/yourâ€‘org/creditâ€‘dataâ€‘generator)
![K8s](https://img.shields.io/badge/kubernetesâ€‘ready-âœ”ï¸â€‘blue)

---

## Table of Contents
1. [Overview](#overview)
2. [Key Features](#key-features)
3. [Architecture](#architecture)
4. [QuickÂ Start](#quick-start)
5. [Deployment Guide](#deployment-guide)
6. [CI/CD Pipeline](#cicd-pipeline)
7. [TechÂ Stack](#tech-stack)
8. [Repository Layout](#repository-layout)
9. [Contributing](#contributing)
10. [License](#license)

---

## Overview
This project **automatically generates highâ€‘fidelity, fullyâ€‘anonymised credit datasets** using a *Generative Adversarial Network* (GAN) trained on domainâ€‘specific credit features. The data generation is orchestrated by a *bigâ€‘data pipeline* that scales elastically on Kubernetes and is fully reproducible via Infrastructureâ€‘asâ€‘Code (IaC).

> **Why?**
> - Enable dataâ€‘hungry modelling teams to experiment safely with synthetic credit data.
> - Provide an opinionated, endâ€‘toâ€‘end template to practise K8s, MLOps and cloud engineering.

---

## Key Features
- **Custom GAN Model** â€“ PyTorchâ€‘based architecture tuned for tabular credit features.
- **Scalable Data Pipeline** â€“ Apache Spark (or Beam) jobs packaged as Docker images and run as K8s jobs.
- **Endâ€‘toâ€‘End MLOps** â€“ Training, validation, and inference orchestrated by Kubeflow Pipelines.
- **IaC First** â€“ All cloud resources (VPC, K8s cluster, storage buckets) provisioned via Terraform.
- **CI/CD** â€“ GitHub Actions deploys code, containers, and infra on every push.
- **Observability** â€“ Prometheus + Grafana dashboards for model & pipeline metrics.
- **Security & Compliance** â€“ Synthetic data ensures no PII leakage; secrets managed withÂ Vault.

---

## Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Source Data  â”‚â”€â”€â–¶â”€â”€â–¶â”‚   GAN Trainer  â”‚â”€â”€â–¶â”€â”€â–¶â”‚ Model Registry    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â–²                             â”‚
                           â”‚                             â”‚
                           â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚           â”‚   Data Generation Spark Jobs      â”‚
                           â”‚           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚                             â”‚
                           â”‚                             â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Feature    â”‚               â”‚ Object      â”‚
                    â”‚  Store      â”‚               â”‚ Storage     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
> SeeÂ `docs/architecture.png` for the full diagram.

---

## QuickÂ Start
```bash
# 1. Clone & enter repo
$ git clone https://github.com/yourâ€‘org/creditâ€‘dataâ€‘generator.git
$ cd creditâ€‘dataâ€‘generator

# 2. Spin up a local K8s cluster (kind or minikube)
$ make k8sâ€‘up

# 3. Build & load Docker images
$ make dockerâ€‘build

# 4. Run a smoke test that trains a tiny GAN and generates 1Â k records
$ make quickâ€‘start
```
The command above launches a Kubeflow pipeline that:
1. Ingests a sample CSV.
2. Trains the GAN for 2Â epochs.
3. Generates 1â€¯000 synthetic rows into `./output/synthetic.csv`.

---

## Deployment Guide
### Prerequisites
| Tool | MinimumÂ Version | Purpose |
|------|-----------------|---------|
| Terraform |Â 1.7Â |Â IaC provisioning |
| kubectl | 1.30 | K8s control |
| Helm |Â 3.15Â |Â Package charts |
| Docker |Â 26.1Â | Container runtime |
| Python |Â 3.11Â | GAN training & CLI |

###Â 1ï¸âƒ£Â ProvisionÂ Infra
```bash
# Edit variables.tf to match your cloud account
$ terraform init && terraform apply
```
###Â 2ï¸âƒ£Â DeployÂ KubernetesÂ Stack
```bash
$ helm dependency update charts/platform
$ helm install platform charts/platform -n platform --create-namespace
```
###Â 3ï¸âƒ£Â KickÂ offÂ Pipeline
```bash
$ kubectl apply -f k8s/pipelines/runâ€‘full.yaml
```
Monitor progress in *Kubeflow Central Dashboard â†’ Runs*.

---

## CI/CD Pipeline
| Stage | Trigger | WhatÂ Happens |
|-------|---------|--------------|
| **Lint & Test** | PRÂ open/push | `pytest`, `ruff`, `yamllint` |
| **BuildÂ Images** | MergeÂ â†’Â main | Build & push versioned Docker images â”‚
| **ApplyÂ IaC** | TagÂ release | Terraform plan + apply to prodÂ account |
| **DeployÂ K8s** | TagÂ release | Helm upgrade of live cluster |

Workflows live in `.github/workflows/` â€“ feel free to copy or adapt.

---

## TechÂ Stack
- **Machine Learning**: PythonÂ 3 Â· PyTorchÂ 2 Â· scikitâ€‘learn Â· WeightsÂ &Â Biases
- **DataÂ Processing**: ApacheÂ Spark 4 / GoogleÂ Dataflow Â· Parquet
- **Orchestration**: Kubeflow Pipelines Â· Argo Workflows
- **Infrastructure**: KubernetesÂ 1.30 Â· TerraformÂ v1 Â· HelmÂ v3
- **CI/CD**: GitHub Actions Â· Docker Buildx
- **Observability**: Prometheus Â· Grafana Â· Loki Â· Jaeger

---

## Repository Layout
```
.
â”œâ”€â”€ charts/            # Helm charts
â”œâ”€â”€ data/              # Sample raw datasets (small)
â”œâ”€â”€ docs/              # Diagrams & ADRs
â”œâ”€â”€ kubeflow/          # Pipeline definitions
â”œâ”€â”€ model/             # GAN architecture & training scripts
â”œâ”€â”€ pipeline/          # Spark jobs & DAG code
â”œâ”€â”€ terraform/         # IaC modules
â””â”€â”€ .github/           # CI/CD workflows
```

---

## Contributing
1. **Fork** the repository and create your feature branch (`git checkout -b feat/amazingâ€‘feature`).
2. Run `make preâ€‘commit` to ensure tests & linters pass.
3. Submit a Pull Request â€“ describe *what* & *why* clearly.

All contributions (code, docs, testing) are welcome! See `CONTRIBUTING.md` for details.

---

## License
Distributed under the **MIT License**. See `LICENSE` for more information.

---

> Â©Â 2025Â Giap Nguyen â€” *Built for learning, shared with â¤ï¸.*
